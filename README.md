# Emirhans_Method_Deepfake
BLIP is a VLP (Vision-Language Pre-training) framework that excels in both vision-language understanding and generation tasks using a multimodal mixture of encoder-decoder model and a novel captioning and filtering method for handling noisy data. In this project, we will try to use BLIP in deepfake detection

# Dataset 

The dataset consists of 300 random examples from the FF++ dataset. This included 150 Deepfake examples and 150 Real examples, selected from 50 videos. Each video contributed 3 frames to the dataset. The dataset can be accessed from the link: https://www.kaggle.com/datasets/emirhanbilgic/ff-blip-dataset 
